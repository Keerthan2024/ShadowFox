import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

print("=== Car Price Prediction - Phase 2: Feature Engineering ===\n")

# Load the dataset
try:
    data = pd.read_csv('data/car_data.csv')
    print("‚úÖ Data loaded successfully")
    print(f"Original shape: {data.shape}")
except Exception as e:
    print(f"‚ùå Error: {e}")
    exit()

# Display original column names
print(f"\nüìã Original column names: {list(data.columns)}")
print(f"\nüîç First 5 rows:")
print(data.head())

# Clean column names
original_columns = data.columns.tolist()
data.columns = data.columns.str.strip().str.replace(' ', '_').str.lower()
print(f"\nüìã Cleaned column names: {list(data.columns)}")

# Show data types
print(f"\nüîç Data types:")
print(data.dtypes)

# Data cleaning
print(f"\nüßπ Data Cleaning:")
print(f"Missing values before cleaning: {data.isnull().sum().sum()}")
initial_shape = data.shape[0]
data = data.dropna()
data = data.drop_duplicates()
final_shape = data.shape[0]
print(f"Rows removed: {initial_shape - final_shape}")
print(f"Final dataset shape: {data.shape}")

# Identify columns by pattern matching
price_cols = [col for col in data.columns if any(word in col.lower() for word in ['price', 'selling'])]
year_cols = [col for col in data.columns if 'year' in col.lower()]
km_cols = [col for col in data.columns if any(word in col.lower() for word in ['km', 'driven', 'mileage'])]
fuel_cols = [col for col in data.columns if 'fuel' in col.lower()]
owner_cols = [col for col in data.columns if 'owner' in col.lower()]
transmission_cols = [col for col in data.columns if 'transmission' in col.lower()]

print(f"\nüéØ Auto-detected columns:")
print(f"Price columns: {price_cols}")
print(f"Year columns: {year_cols}")
print(f"KM columns: {km_cols}")
print(f"Fuel columns: {fuel_cols}")
print(f"Owner columns: {owner_cols}")
print(f"Transmission columns: {transmission_cols}")

# Feature engineering
print(f"\n‚öôÔ∏è Feature Engineering:")
if year_cols:
    year_col = year_cols[0]
    current_year = 2024
    data['car_age'] = current_year - data[year_col]
    print(f"‚úÖ Created 'car_age' from '{year_col}'")

# Create price per km feature if both exist
if price_cols and km_cols:
    price_col = price_cols[0]
    km_col = km_cols[0]
    data['price_per_km'] = data[price_col] / (data[km_col] + 1)  # +1 to avoid division by zero
    print(f"‚úÖ Created 'price_per_km' feature")

# Create depreciation rate if both selling_price and present_price exist
if 'selling_price' in data.columns and 'present_price' in data.columns:
    data['depreciation_rate'] = (data['present_price'] - data['selling_price']) / data['present_price']
    print(f"‚úÖ Created 'depreciation_rate' feature")

# Create visualizations with perfect spacing
plt.rcParams.update({'font.size': 10})
fig = plt.figure(figsize=(18, 12))

# Create subplot grid with optimal spacing
gs = fig.add_gridspec(2, 2, hspace=0.35, wspace=0.25, 
                      left=0.08, right=0.95, top=0.92, bottom=0.08)

# 1. Price Distribution
ax1 = fig.add_subplot(gs[0, 0])
if price_cols:
    price_col = price_cols[0]
    n, bins, patches = ax1.hist(data[price_col], bins=30, edgecolor='black', alpha=0.7, color='skyblue')
    ax1.set_title('Price Distribution', fontsize=14, fontweight='bold', pad=15)
    ax1.set_xlabel('Selling Price (Lakhs)', fontsize=11)
    ax1.set_ylabel('Frequency', fontsize=11)
    ax1.grid(True, alpha=0.3)
    
    # Add statistics text
    mean_price = data[price_col].mean()
    median_price = data[price_col].median()
    ax1.axvline(mean_price, color='red', linestyle='--', linewidth=2, label=f'Mean: ‚Çπ{mean_price:.1f}L')
    ax1.axvline(median_price, color='green', linestyle='--', linewidth=2, label=f'Median: ‚Çπ{median_price:.1f}L')
    ax1.legend(fontsize=9)
else:
    ax1.text(0.5, 0.5, 'Price column not found', ha='center', va='center', fontsize=12)
    ax1.set_title('Price Distribution - Not Available')

# 2. Car Age vs Price
ax2 = fig.add_subplot(gs[0, 1])
if 'car_age' in data.columns and price_cols:
    price_col = price_cols[0]
    scatter = ax2.scatter(data['car_age'], data[price_col], alpha=0.6, color='orange', s=25)
    ax2.set_title('Car Age vs Selling Price', fontsize=14, fontweight='bold', pad=15)
    ax2.set_xlabel('Car Age (years)', fontsize=11)
    ax2.set_ylabel('Selling Price (Lakhs)', fontsize=11)
    ax2.grid(True, alpha=0.3)
    
    # Add trend line
    z = np.polyfit(data['car_age'], data[price_col], 1)
    p = np.poly1d(z)
    ax2.plot(data['car_age'], p(data['car_age']), "r--", alpha=0.8, linewidth=2, label='Trend')
    ax2.legend(fontsize=9)
else:
    ax2.text(0.5, 0.5, 'Age/Price data unavailable', ha='center', va='center', fontsize=12)
    ax2.set_title('Age vs Price - Not Available')

# 3. KM vs Price
ax3 = fig.add_subplot(gs[1, 0])
if km_cols and price_cols:
    km_col = km_cols[0]
    price_col = price_cols[0]
    ax3.scatter(data[km_col]/1000, data[price_col], alpha=0.6, color='green', s=25)  # Convert to thousands
    ax3.set_title('KM Driven vs Price', fontsize=14, fontweight='bold', pad=15)
    ax3.set_xlabel('KM Driven (Thousands)', fontsize=11)
    ax3.set_ylabel('Selling Price (Lakhs)', fontsize=11)
    ax3.grid(True, alpha=0.3)
    
    # Add trend line
    z = np.polyfit(data[km_col], data[price_col], 1)
    p = np.poly1d(z)
    ax3.plot(data[km_col]/1000, p(data[km_col]), "r--", alpha=0.8, linewidth=2, label='Trend')
    ax3.legend(fontsize=9)
else:
    ax3.text(0.5, 0.5, 'KM/Price data unavailable', ha='center', va='center', fontsize=12)
    ax3.set_title('KM vs Price - Not Available')

# 4. Fuel Type Distribution
ax4 = fig.add_subplot(gs[1, 1])
if fuel_cols:
    fuel_col = fuel_cols[0]
    fuel_counts = data[fuel_col].value_counts()
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FFEAA7']
    bars = ax4.bar(range(len(fuel_counts)), fuel_counts.values, 
                   color=colors[:len(fuel_counts)], alpha=0.8, edgecolor='black', linewidth=1)
    ax4.set_title('Fuel Type Distribution', fontsize=14, fontweight='bold', pad=15)
    ax4.set_xlabel('Fuel Type', fontsize=11)
    ax4.set_ylabel('Count', fontsize=11)
    ax4.set_xticks(range(len(fuel_counts)))
    ax4.set_xticklabels(fuel_counts.index, rotation=0, fontsize=10)
    ax4.grid(True, alpha=0.3, axis='y')
    
    # Add value labels on bars
    for bar, value in zip(bars, fuel_counts.values):
        height = bar.get_height()
        ax4.text(bar.get_x() + bar.get_width()/2, height + max(fuel_counts.values)*0.01, 
                str(value), ha='center', va='bottom', fontsize=10, fontweight='bold')
    
    # Add percentage labels
    total = sum(fuel_counts.values)
    for i, (bar, value) in enumerate(zip(bars, fuel_counts.values)):
        percentage = (value/total)*100
        ax4.text(bar.get_x() + bar.get_width()/2, height/2, 
                f'{percentage:.1f}%', ha='center', va='center', 
                fontsize=9, color='white', fontweight='bold')
else:
    ax4.text(0.5, 0.5, 'Fuel data unavailable', ha='center', va='center', fontsize=12)
    ax4.set_title('Fuel Distribution - Not Available')

plt.suptitle('üöó Car Price Dataset Analysis', fontsize=18, fontweight='bold', y=0.96)
plt.show()

# Dataset summary
print(f"\nüìä Dataset Summary:")
print(f"Total records: {len(data):,}")
categorical_cols = data.select_dtypes(include=['object']).columns
numerical_cols = data.select_dtypes(include=[np.number]).columns
print(f"Categorical columns ({len(categorical_cols)}): {list(categorical_cols)}")
print(f"Numerical columns ({len(numerical_cols)}): {list(numerical_cols)}")

# Detailed categorical analysis
print(f"\nüè∑Ô∏è Categorical Data Analysis:")
for col in categorical_cols:
    unique_count = data[col].nunique()
    print(f"\nüìã {col.upper()}:")
    print(f"   Unique values: {unique_count}")
    
    value_counts = data[col].value_counts()
    if unique_count <= 15:
        print(f"   Distribution:")
        for value, count in value_counts.items():
            percentage = (count/len(data))*100
            print(f"     ‚Ä¢ {value}: {count} ({percentage:.1f}%)")
    else:
        print(f"   Top 5 most common:")
        for value, count in value_counts.head().items():
            percentage = (count/len(data))*100
            print(f"     ‚Ä¢ {value}: {count} ({percentage:.1f}%)")

# Enhanced correlation analysis with perfect display
print(f"\nüîó Correlation Analysis:")
if len(numerical_cols) > 1:
    # Calculate optimal figure size
    n_features = len(numerical_cols)
    fig_size = max(10, min(16, n_features * 1.5))
    
    plt.figure(figsize=(fig_size, fig_size))
    
    correlation_matrix = data[numerical_cols].corr()
    
    # Create heatmap with perfect settings
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)  # Mask upper triangle
    
    sns.heatmap(correlation_matrix, 
                mask=mask,            # Show only lower triangle
                annot=True,           # Show correlation values
                cmap='RdBu_r',        # Red-Blue colormap (reversed)
                center=0,             # Center colormap at 0
                square=True,          # Make cells square
                linewidths=1,         # Lines between cells
                fmt='.2f',            # 2 decimal places
                annot_kws={'size': 11, 'weight': 'bold'},  # Font settings
                cbar_kws={"shrink": 0.8, "aspect": 30, "pad": 0.1})
    
    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold', pad=25)
    plt.xticks(rotation=45, ha='right', fontsize=12, fontweight='bold')
    plt.yticks(rotation=0, fontsize=12, fontweight='bold')
    
    # Perfect layout adjustment
    plt.tight_layout()
    plt.subplots_adjust(bottom=0.2, left=0.2, right=0.85, top=0.9)
    
    plt.show()
    
    # Correlation insights
    print(f"\nüîç Key Correlation Insights:")
    
    # Find target column correlations
    target_col = None
    for col in ['selling_price', 'price']:
        if col in numerical_cols:
            target_col = col
            break
    
    if target_col:
        price_corr = correlation_matrix[target_col].abs().sort_values(ascending=False)
        print(f"üìä Features most correlated with {target_col}:")
        for feature, corr in price_corr.items():
            if feature != target_col and abs(corr) > 0.1:
                direction = "positively" if correlation_matrix[target_col][feature] > 0 else "negatively"
                strength = "Very Strong" if abs(corr) > 0.8 else "Strong" if abs(corr) > 0.6 else "Moderate" if abs(corr) > 0.4 else "Weak"
                print(f"   ‚Ä¢ {feature}: {corr:.3f} ({strength} {direction} correlated)")
    
    # Multicollinearity check
    print(f"\nüö® Multicollinearity Check (|correlation| > 0.8):")
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > 0.8:
                high_corr_pairs.append((correlation_matrix.columns[i], 
                                      correlation_matrix.columns[j], 
                                      corr_val))
    
    if high_corr_pairs:
        for feat1, feat2, corr in high_corr_pairs:
            print(f"   ‚ö†Ô∏è  {feat1} ‚Üî {feat2}: {corr:.3f}")
        print(f"   üí° Consider removing one feature from highly correlated pairs")
    else:
        print("   ‚úÖ No concerning multicollinearity detected")

# Encode categorical variables
print(f"\nüî§ Encoding Categorical Variables:")
label_encoders = {}

for col in categorical_cols:
    if col not in price_cols:
        le = LabelEncoder()
        data[f'{col}_encoded'] = le.fit_transform(data[col].astype(str))
        label_encoders[col] = le
        print(f"‚úÖ Encoded '{col}' -> '{col}_encoded'")
        print(f"   Mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}")

# Model preparation
target_col = None
for potential_target in ['selling_price', 'price']:
    if potential_target in data.columns:
        target_col = potential_target
        break

if target_col:
    print(f"\nüéØ Target Variable: '{target_col}'")
    
    # Select features intelligently
    feature_cols = []
    
    # Add numerical features (except target and highly correlated features)
    excluded_features = [target_col]
    if 'car_age' in data.columns and 'year' in data.columns:
        excluded_features.append('year')  # Remove year if we have car_age
        
    for col in numerical_cols:
        if col not in excluded_features:
            feature_cols.append(col)
    
    # Add encoded categorical features
    for col in categorical_cols:
        if f'{col}_encoded' in data.columns:
            feature_cols.append(f'{col}_encoded')
    
    print(f"\nüìä Selected Features ({len(feature_cols)}):")
    for i, feat in enumerate(feature_cols, 1):
        print(f"   {i:2d}. {feat}")
    
    # Create feature matrix and target
    X = data[feature_cols]
    y = data[target_col]
    
    print(f"\n‚úÖ Model Data Prepared:")
    print(f"   Features shape: {X.shape}")
    print(f"   Target shape: {y.shape}")
    
    # Target variable analysis
    print(f"\nüìà Target Variable Statistics:")
    print(f"   Mean price: ‚Çπ{y.mean():.2f} Lakh")
    print(f"   Median price: ‚Çπ{y.median():.2f} Lakh")
    print(f"   Price range: ‚Çπ{y.min():.2f} - ‚Çπ{y.max():.2f} Lakh")
    print(f"   Standard deviation: ‚Çπ{y.std():.2f} Lakh")
    print(f"   Coefficient of variation: {(y.std()/y.mean())*100:.1f}%")
    
    # Outlier analysis
    Q1 = y.quantile(0.25)
    Q3 = y.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    outliers = ((y < lower_bound) | (y > upper_bound)).sum()
    print(f"   Outliers detected: {outliers} ({(outliers/len(y))*100:.1f}%)")
    
    # Train-test split with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, shuffle=True
    )
    
    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    print(f"\n‚úÖ Data Split & Scaling Complete:")
    print(f"   Training samples: {X_train_scaled.shape[0]} ({(X_train_scaled.shape[0]/len(X))*100:.1f}%)")
    print(f"   Testing samples: {X_test_scaled.shape[0]} ({(X_test_scaled.shape[0]/len(X))*100:.1f}%)")
    print(f"   Features: {X_train_scaled.shape[1]}")
    print(f"   Training target range: ‚Çπ{y_train.min():.2f} - ‚Çπ{y_train.max():.2f} Lakh")
    print(f"   Testing target range: ‚Çπ{y_test.min():.2f} - ‚Çπ{y_test.max():.2f} Lakh")
    
    # Save all preprocessing artifacts
    import joblib
    
    # Save main preprocessing data
    joblib.dump((X_train_scaled, X_test_scaled, y_train, y_test, scaler, feature_cols), 
                'car_data_preprocessed.pkl')
    
    # Save label encoders
    joblib.dump(label_encoders, 'label_encoders.pkl')
    
    # Save processed dataset
    joblib.dump(data, 'processed_dataset.pkl')
    
    # Save feature information for API
    feature_info = {}
    for col in data.columns:
        if col in categorical_cols:
            feature_info[col] = {
                'type': 'categorical',
                'unique_values': data[col].unique().tolist(),
                'most_common': data[col].mode()[0]
            }
        elif col in numerical_cols:
            feature_info[col] = {
                'type': 'numerical',
                'min': float(data[col].min()),
                'max': float(data[col].max()),
                'mean': float(data[col].mean()),
                'std': float(data[col].std())
            }
    
    joblib.dump(feature_info, 'feature_info.pkl')
    
    print(f"\nüíæ Files Saved Successfully:")
    print("   ‚úÖ car_data_preprocessed.pkl (Training data)")
    print("   ‚úÖ label_encoders.pkl (Categorical encoders)")
    print("   ‚úÖ processed_dataset.pkl (Full processed dataset)")
    print("   ‚úÖ feature_info.pkl (Feature metadata)")

else:
    print("‚ùå No suitable target variable found!")
    print("Available columns:", list(data.columns))

print(f"\n" + "="*60)
print("üéâ PHASE 2 COMPLETE!")
print("="*60)
print("üìä Data preprocessing and feature engineering finished successfully")
print("üöÄ Next step: Run 'python model_training.py' to train ML models")
print("‚è±Ô∏è  Expected training time: 2-5 minutes")
print("üéØ Expected model accuracy: 85-95% (based on feature quality)")
